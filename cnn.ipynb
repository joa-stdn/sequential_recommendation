{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CNN for recommendation"
      ],
      "metadata": {
        "id": "O_mBSw7zb0Ba"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Clnpei9Wf4Zv"
      },
      "source": [
        "from keras.layers import Dense, Activation, Input, MaxPooling2D, Embedding, Conv2D, Reshape, Permute\n",
        "from keras.models import load_model, Model, Sequential\n",
        "from keras.utils import Sequence\n",
        "from keras.utils import plot_model, model_to_dot\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import csv\n",
        "import math\n",
        "import numpy as np\n",
        "# from keras_sequential_ascii import keras2ascii"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The goal of this notebook is to leverage the geometrical representation power of CNNs to achieve better performance on recommendation on a series of benchmarks."
      ],
      "metadata": {
        "id": "kt2YG9RFboVz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive # Il faut pouvoir lire les fichiers CSV du Drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "8Wmd5jsjfNWg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dSk7VcF0fxuU"
      },
      "source": [
        "# pip install git+git://github.com/stared/keras-sequential-ascii.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtvBDYucgq25"
      },
      "source": [
        "# Nombre de valeurs dans le dictionnaire + la valeur vide\n",
        "d = 14370 + 1 # 1 catégories supplémentaires : une <EOS>\n",
        "# Taille de l'input\n",
        "Tx = 64\n",
        "# Taille de l'output\n",
        "Ty = 16\n",
        "# Dimension de l'embedding\n",
        "n_e = 256\n",
        "# Mini-batch size\n",
        "m = 256\n",
        "\n",
        "# Paramètre du premier CONV2D\n",
        "n_f_1 , n_k_1 = 32 , 3\n",
        "n_f_2 , n_k_2 = 64 , 5\n",
        "n_f_3 , n_k_3 = 128 , 7\n",
        "\n",
        "n_mp_1 = 2\n",
        "n_mp_2 = 4\n",
        "n_mp_3 = 8\n",
        "\n",
        "n_k_4 = 32\n",
        "\n",
        "# Paramêtres du FC network\n",
        "n_fc = 512"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIyllHQxgc3Q"
      },
      "source": [
        "# def CNNmodel():\n",
        "#   inpt = Input(shape=(Tx,))\n",
        "#   # Embedding\n",
        "#   embedding = Embedding(input_dim=d , output_dim=n_e, input_length=Tx)\n",
        "#   # First layer, we embedd the input sequence to get a (Tx , n_e) picture\n",
        "#   x = embedding(inpt)\n",
        "#   x = Reshape(target_shape=(Tx, n_e , 1))(x)\n",
        "\n",
        "#   # A Conv2D network\n",
        "#   x = Conv2D(filters=n_f_1 , kernel_size= n_k_1, padding = \"same\" , activation=\"relu\")(x)\n",
        "#   x = MaxPooling2D(pool_size=(n_mp_1 , 1) , padding=\"same\")(x)\n",
        "#   x = Conv2D(filters=n_f_2 , kernel_size= n_k_2, padding = \"same\", activation=\"relu\")(x)\n",
        "#   x = MaxPooling2D(pool_size=(n_mp_2 , 1) , padding=\"same\")(x)\n",
        "#   x = Conv2D(filters=n_f_3 , kernel_size= n_k_3, padding = \"same\", activation=\"relu\")(x)\n",
        "#   x = MaxPooling2D(pool_size=(n_mp_3 , 1) , padding=\"same\")(x)\n",
        "#   x = Conv2D(filters=Ty , kernel_size= (1, n_k_4), padding = \"same\", activation=\"relu\")(x)\n",
        "\n",
        "#   x = Reshape(target_shape=(n_e, Ty))(x)\n",
        "#   x = Permute((2,1) , input_shape=(n_e,Ty))(x)\n",
        "#   # A FC network\n",
        "#   x = Dense(n_fc, activation=\"relu\")(x)\n",
        "#   x = Dense(d , activation=\"softmax\")(x)\n",
        "\n",
        "#   model = Model(inputs = inpt , outputs = x)\n",
        "#   return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30RrxHZDj8cu"
      },
      "source": [
        "# model = CNNmodel()\n",
        "# model.compile(loss=\"sparse_categorical_crossentropy\",optimizer=\"adam\", metrics=\"accuracy\")\n",
        "#model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COoQ2OX4vgOC"
      },
      "source": [
        "#model.save(\"/content/drive/MyDrive/PSC Recommandation séquentielle/Modèles/CNN/NoPairCNN_save\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "suhucVKYvF84"
      },
      "source": [
        "folder = \"/content/drive/MyDrive/PSC Recommandation séquentielle/Données/DataTables/\"\n",
        "class DataGenerator(Sequence):\n",
        "  def __init__(self , nb_lines, X_path, Y_path):\n",
        "    self.X_path = X_path\n",
        "    self.X_reader = csv.reader(open(folder + X_path , \"r\"))\n",
        "    self.Y_path = Y_path\n",
        "    self.Y_reader = csv.reader(open(folder + Y_path , \"r\"))\n",
        "    self.nb_lines = nb_lines\n",
        "\n",
        "  def __len__(self):\n",
        "    return math.ceil(self.nb_lines/m)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    X1 = []\n",
        "    Y = []\n",
        "    for i in range(m):\n",
        "      x,y = self.getNextSample()\n",
        "      x = [int(i) for i in x]\n",
        "      y = [[int(i)] for i in y]\n",
        "      X1.append(x)\n",
        "      Y.append(y)\n",
        "    X1 = np.array(X1)\n",
        "    return np.array(X1) , np.array(Y)\n",
        "\n",
        "  def getNextSample(self):\n",
        "    x = next(self.X_reader , None)\n",
        "    y = next(self.Y_reader , None)\n",
        "    if x is None:\n",
        "      self.X_reader = csv.reader(open(folder + self.X_path , \"r\"))\n",
        "      self.Y_reader = csv.reader(open(folder + self.Y_path , \"r\"))\n",
        "      x = next(self.X_reader , None)\n",
        "      y = next(self.Y_reader , None)\n",
        "    return x , y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BzBJ6aTal-mN"
      },
      "source": [
        "model = load_model(\"/content/drive/MyDrive/PSC Recommandation séquentielle/Modèles/CNN/CNN No Pair/NoPairCNN_save\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# graph = model_to_dot(model)\n",
        "# graph.write_png('/content/drive/MyDrive/PSC Recommandation séquentielle/Modèles/CNN/CNN No Pair/model.png')\n",
        "# model.summary()"
      ],
      "metadata": {
        "id": "vp1ZrG8HfPNo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_gen = DataGenerator(3705954\n",
        "#                           , \"./s6_X_train.csv\"\n",
        "#                           ,\"./s6_Y_train.csv\")\n",
        "# model.fit(train_gen , epochs=1, verbose = 1)\n",
        "\n",
        "# model.save(\"NoPairCNN_save\")"
      ],
      "metadata": {
        "id": "pPU04VJxfQYv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zehx9Ggmiya7"
      },
      "source": [
        "# Comparing train and dev sets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_gen = DataGenerator(3705954, \"s_X_train.csv\" , \"s_Y_train.csv\")\n",
        "model.evaluate(train_gen)"
      ],
      "metadata": {
        "id": "ApGoSR30b6ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrchNsvDjBRn"
      },
      "source": [
        "dev_gen = DataGenerator(205412, \"X_dev.csv\" , \"Y_dev.csv\")\n",
        "model.evaluate(dev_gen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(folder + \"X_dev.csv\", 'r') as csvfile:\n",
        "  spamreader = csv.reader(csvfile)\n",
        "  n = 0\n",
        "  for row in spamreader:\n",
        "    n += 1\n",
        "  print(n) # 206202 lignes dans le dev set, 205412 dans le test set, 3705954 dans le train set"
      ],
      "metadata": {
        "id": "v2tRsoYOfRgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZnzG9lN5BQd"
      },
      "source": [
        "## Kind of beam search to predict baskets in the most promising way"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIMj1vAk5Cjd"
      },
      "source": [
        "def k_largest(l, k, exclus = []): # renvoie une liste ind des indices des k plus grands éléments de l avec l[ind[0]] >= ... >= l[ind[k-1]]. exclus est une liste d'indices exclus de la recherche\n",
        "  n = len(l)\n",
        "  if (n - len(exclus) <= k):\n",
        "    ind = [i for i in range (len(l)) if i not in exclus] # on n'oublie pas de gérer les exclus\n",
        "    ind.sort(key = lambda i: -l[i])\n",
        "    return ind\n",
        "  exclus1 = [i for i in exclus if i < n // 2]\n",
        "  exclus2 = [i - n // 2 for i in exclus if i >= n // 2]\n",
        "  ind_temp1 = k_largest(l[:n // 2], k, exclus1)\n",
        "  ind_temp2 = k_largest(l[n // 2:], k, exclus2)\n",
        "  k1, k2 = len(ind_temp1), len(ind_temp2)\n",
        "  i1, i2 = 0, 0\n",
        "  ind = []\n",
        "  while (i1 + i2 < k):\n",
        "    if (i2 == k2 or (i1 < k1 and l[ind_temp1[i1]] > l[ind_temp2[i2] + n // 2])):\n",
        "      ind.append(ind_temp1[i1])\n",
        "      i1 += 1\n",
        "    else:\n",
        "      ind.append(ind_temp2[i2] + n // 2)\n",
        "      i2 += 1\n",
        "  return ind"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uONoHBVj5ETj"
      },
      "source": [
        "def predict(y_hat, k):\n",
        "  # on suppose que y_hat est déjà de dimension 16 * (d - 1) où d - 1 = 14370\n",
        "  ran = []\n",
        "  for i0 in range (k): ran.extend([(i0, j) for j in range (k)])\n",
        "  pred = [] # on initialise la prédiction\n",
        "  ind1 = k_largest(y_hat[0], k, pred)\n",
        "  for i in range (15):\n",
        "    ind2 = k_largest(y_hat[i + 1], k, pred)\n",
        "    ind = [(ind1[i1], ind2[i2], y_hat[i][ind1[i1]] * y_hat[i + 1][ind2[i2]]) for (i1, i2) in ran]\n",
        "    ind.sort(key = lambda j: -j[2])\n",
        "    l = min(j for j in range (k ** 2) if (ind[j][0] != ind[j][1]))\n",
        "    pred.append(ind[l][0])\n",
        "    if (i == 14): pred.append(ind[l][1])\n",
        "    else: ind1 = k_largest(y_hat[i + 1], k, pred)\n",
        "  return pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4aQ8B-G5IRV"
      },
      "source": [
        "def predict2(y_hat, k):\n",
        "  # on suppose que y_hat est déjà de dimension 16 * (d - 1) où d - 1 = 14370\n",
        "  ran = []\n",
        "  for i0 in range (k): ran.extend([(i0, j) for j in range (k)])\n",
        "  pred = [] # on initialise la prédiction\n",
        "  for i in range (0, 15, 2):\n",
        "    ind1 = k_largest(y_hat[i], k, pred)\n",
        "    ind2 = k_largest(y_hat[i + 1], k, pred)\n",
        "    ind = [(ind1[i1], ind2[i2], y_hat[i][ind1[i1]] * y_hat[i + 1][ind2[i2]]) for (i1, i2) in ran]\n",
        "    ind.sort(key = lambda j: -j[2])\n",
        "    l = min(j for j in range (k ** 2) if (ind[j][0] != ind[j][1]))\n",
        "    pred.append(ind[l][0])\n",
        "    pred.append(ind[l][1])\n",
        "  return pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKPu08DB5Ky0"
      },
      "source": [
        "def predict3(y_hat, k):\n",
        "  # on suppose que y_hat est déjà de dimension 16 * (d - 1) où d - 1 = 14370\n",
        "  ran = []\n",
        "  for i1 in range (k):\n",
        "    for i2 in range (k):\n",
        "      for i3 in range (k):\n",
        "        ran.append((i1, i2, i3))\n",
        "  pred = [] # on initialise la prédiction\n",
        "  for i in range (0, 15, 3):\n",
        "    ind1 = k_largest(y_hat[i], k, pred)\n",
        "    ind2 = k_largest(y_hat[i + 1], k, pred)\n",
        "    ind3 = k_largest(y_hat[i + 2], k, pred)\n",
        "    ind = [(ind1[i1], ind2[i2], ind3[i3], y_hat[i][ind1[i1]] * y_hat[i + 1][ind2[i2]] * y_hat[i + 2][ind3[i3]]) for (i1, i2, i3) in ran]\n",
        "    ind.sort(key = lambda j: -j[3])\n",
        "    l = min(j for j in range (k ** 3) if (ind[j][0] != ind[j][1] and ind[j][0] != ind[j][2] and ind[j][1] != ind[j][2]))\n",
        "    pred.append(ind[l][0])\n",
        "    pred.append(ind[l][1])\n",
        "    pred.append(ind[l][2])\n",
        "    if (i == 12): pred.append(k_largest(y_hat[i + 3], 2, pred)[0]) # on ajoute le dernier\n",
        "  return pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I08MS6jH5TCp"
      },
      "source": [
        "def predict4(y_hat, k):\n",
        "  # on suppose que y_hat est déjà de dimension 16 * (d - 1) où d - 1 = 14370\n",
        "  ran = []\n",
        "  for i1 in range (k):\n",
        "    for i2 in range (k):\n",
        "      for i3 in range (k):\n",
        "        for i4 in range (k):\n",
        "          ran.append((i1, i2, i3, i4))\n",
        "  pred = [] # on initialise la prédiction\n",
        "  for i in range (0, 15, 4):\n",
        "    ind1 = k_largest(y_hat[i], k, pred)\n",
        "    ind2 = k_largest(y_hat[i + 1], k, pred)\n",
        "    ind3 = k_largest(y_hat[i + 2], k, pred)\n",
        "    ind4 = k_largest(y_hat[i + 3], k, pred)\n",
        "    ind = [(ind1[i1], ind2[i2], ind3[i3], ind4[i4], y_hat[i][ind1[i1]] * y_hat[i + 1][ind2[i2]] * y_hat[i + 2][ind3[i3]] * y_hat[i + 3][ind4[i4]]) for (i1, i2, i3, i4) in ran]\n",
        "    ind.sort(key = lambda j: -j[4])\n",
        "    l = min(j for j in range (k ** 4) if (ind[j][0] != ind[j][1] and ind[j][0] != ind[j][2] and ind[j][1] != ind[j][2] and ind[j][0] != ind[j][3] and ind[j][1] != ind[j][3] and ind[j][2] != ind[j][3]))\n",
        "    pred.append(ind[l][0])\n",
        "    pred.append(ind[l][1])\n",
        "    pred.append(ind[l][2])\n",
        "    pred.append(ind[l][3])\n",
        "  return pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQHWpOsB5VrA"
      },
      "source": [
        "## Results"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(folder+\"Y_dev.csv\" , \"r\") as file :\n",
        "  r = csv.reader(file)\n",
        "  for i in range(1):\n",
        "    next(r)\n",
        "  y = [int(i) for i in next(r)]\n",
        "  print(\"Séquence de référence : \")\n",
        "  print(y)\n",
        "\n",
        "with open(folder+\"X_dev.csv\" , \"r\") as file :\n",
        "  r = csv.reader(file)\n",
        "  for i in range(1):\n",
        "    next(r)\n",
        "  x = np.array([[ int(i) for i in next(r)]])\n",
        "  print(\"64 achats précédents\")\n",
        "  print(x)\n",
        "  y_hat = model(x)\n",
        "  y_hat = y_hat[0]\n",
        "y_hat_np = y_hat.numpy()[:, 1:] # ATTENTION ON ENLEVE LA PREMIERE COLONNE A CAUSE DU ARGMAX...\n",
        "p_y = [y_hat_np[i][y[i]] for i in range(16)]\n",
        "print(\"Probas de la séquence de référence :\")\n",
        "print(p_y)\n",
        "# p_y_hat = [np.max(y_hat_np[i]) for i in range(16)]\n",
        "# y_hat = [1 + np.argmax(y_hat_np[i]) for i in range(16)]\n",
        "########## PREMIERE METHODE ##########\n",
        "y_hat = []\n",
        "p_y_hat = []\n",
        "for i in range (16):\n",
        "  j = min(l for l in range (d - 1) if l not in y_hat)\n",
        "  for l in range (j, d-1):\n",
        "    if (l not in y_hat and y_hat_np[i][l] > y_hat_np[i][j]):\n",
        "      j = l\n",
        "  y_hat.append(j)\n",
        "  p_y_hat.append(y_hat_np[i][j])\n",
        "y_hat = [1 + y for y in y_hat]\n",
        "print(\"Séquence inférée et proba (méthode simple) : \")\n",
        "print(y_hat)\n",
        "print(p_y_hat)\n",
        "\n",
        "########## DEUXIEME METHODE ##########\n",
        "y_hat = predict(y_hat_np, 20)\n",
        "p_y_hat = [y_hat_np[i][y_hat[i]] for i in range (16)]\n",
        "y_hat = [1 + x for x in y_hat]\n",
        "print(\"Séquence inférée et proba (méthode faisceaux) : \")\n",
        "print(y_hat)\n",
        "print(p_y_hat)\n",
        "\n",
        "########## TROISIEME METHODE ##########\n",
        "y_hat = predict2(y_hat_np, 100)\n",
        "p_y_hat = [y_hat_np[i][y_hat[i]] for i in range (16)]\n",
        "y_hat = [1 + x for x in y_hat]\n",
        "print(\"Séquence inférée et proba (méthode faisceaux deux par deux) : \")\n",
        "print(y_hat)\n",
        "print(p_y_hat)\n",
        "\n",
        "########## QUATRIEME METHODE ##########\n",
        "y_hat = predict3(y_hat_np, 20)\n",
        "p_y_hat = [y_hat_np[i][y_hat[i]] for i in range (16)]\n",
        "y_hat = [1 + x for x in y_hat]\n",
        "print(\"Séquence inférée et proba (méthode faisceaux trois par trois) : \")\n",
        "print(y_hat)\n",
        "print(p_y_hat)\n",
        "\n",
        "########## CINQUIEME METHODE ##########\n",
        "y_hat = predict4(y_hat_np, 15)\n",
        "p_y_hat = [y_hat_np[i][y_hat[i]] for i in range (16)]\n",
        "y_hat = [1 + x for x in y_hat]\n",
        "print(\"Séquence inférée et proba (méthode faisceaux quatre par quatre) : \")\n",
        "print(y_hat)\n",
        "print(p_y_hat)"
      ],
      "metadata": {
        "id": "8DUSflWacFO8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}